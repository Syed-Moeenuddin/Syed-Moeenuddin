{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet('../data/processed/df_train.parquet')\n",
    "test = pd.read_parquet('../data/processed/df_test.parquet')\n",
    "test.loc[test['date'] == test['date'].min(), 'price_change_perc'] = 0\n",
    "test.loc[test['date'] == test['date'].min(), 'price_change_logdiff'] = 0\n",
    "\n",
    "sub = pd.read_csv('../data/raw/sample_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_df(df, max_days=100):\n",
    "    df = df[df['date'] > df['date'].max() - timedelta(days=max_days)]\n",
    "    df = df[['geo_cluster_id', 'sku_id', 'date', 'price_change_perc', 'sales']].copy()\n",
    "    df['price_change'] = df['price_change_perc'] !=0\n",
    "    return df.drop(columns= 'price_change_perc')\n",
    "\n",
    "train = transform_df(train)\n",
    "test = transform_df(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_cluster_id_encoder = LabelEncoder().fit(pd.concat([train['geo_cluster_id'], test['geo_cluster_id']], axis=0))\n",
    "train['geo_cluster_id'] = geo_cluster_id_encoder.transform(train['geo_cluster_id'])\n",
    "test['geo_cluster_id'] = geo_cluster_id_encoder.transform(test['geo_cluster_id'])\n",
    "\n",
    "sku_id_encoder = LabelEncoder().fit(pd.concat([train['sku_id'], test['sku_id']], axis=0))\n",
    "train['sku_id'] = sku_id_encoder.transform(train['sku_id'])\n",
    "test['sku_id'] = sku_id_encoder.transform(test['sku_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(df, target_date, n_target=14):\n",
    "    target = df.loc[(target_date + timedelta(days=n_target) >= df['date']) & (df['date'] >= target_date)\n",
    "        ,['geo_cluster_id', 'sku_id', 'date', 'sales']].copy()\n",
    "    for i in range(1, n_target+1):\n",
    "        target[f'sales_{i}'] = target['sales'].shift(-i)\n",
    "    geo_cluster_mask = target['geo_cluster_id'] == target['geo_cluster_id'].shift(-14)\n",
    "    sku_id_mask = target['sku_id'] == target['sku_id'].shift(-14)\n",
    "    target = target[geo_cluster_mask & sku_id_mask].dropna()\n",
    "    return target.drop(columns='sales')\n",
    "\n",
    "def get_price_change(df, target_date, n_target=14):\n",
    "    target = df.loc[(target_date + timedelta(days=n_target) >= df['date']) & (df['date'] >= target_date)\n",
    "        ,['geo_cluster_id', 'sku_id', 'date', 'price_change']].copy()\n",
    "    for i in range(1, n_target+1):\n",
    "        target[f'price_change_{i}'] = target['price_change'].shift(-i)\n",
    "    geo_cluster_mask = target['geo_cluster_id'] ==target['geo_cluster_id'].shift(-14)\n",
    "    sku_id_mask = target['sku_id'] == target['sku_id'].shift(-14)\n",
    "    target = target[geo_cluster_mask & sku_id_mask].dropna()\n",
    "    \n",
    "    target.loc[:, target.filter(regex=r'^price_change_').columns] = target.filter(regex=r'^price_change_').astype(int)\n",
    "    return target.drop(columns='price_change')\n",
    "\n",
    "def get_feature(df, target_date, timdelta_train_days=30):\n",
    "    df = df[(target_date >= df['date']) & (df['date'] > target_date - timedelta(days=timdelta_train_days))]\n",
    "    \n",
    "    median_sales = df.groupby(['geo_cluster_id', 'sku_id'])['sales'].median().rename('median_sales')\n",
    "    median_non_zero_sales = df[df['sales']!=0].groupby(['geo_cluster_id', 'sku_id'])['sales'].median().rename('median_non_zero_sales')\n",
    "    median_price_change_sales = df[df['price_change']!=0].groupby(['geo_cluster_id', 'sku_id'])['sales'].median().rename('median_price_change_sales')\n",
    "    median_price_no_change_sales = df[df['price_change']==0].groupby(['geo_cluster_id', 'sku_id'])['sales'].median().rename('median_price_no_change_sales')\n",
    "    \n",
    "    result = pd.concat([median_sales, median_non_zero_sales, median_price_change_sales, median_price_no_change_sales], axis=1).fillna(0)\n",
    "    result['date'] = target_date\n",
    "    result = result.merge(df[['geo_cluster_id', 'sku_id', 'date', 'price_change', 'sales']], on=['geo_cluster_id', 'sku_id', 'date'], how='left')\n",
    "    return result \n",
    "\n",
    "def get_df(df, target_date):\n",
    "    target = get_target(df , target_date)\n",
    "    feature = get_feature(df , target_date)\n",
    "    price_change = get_price_change(df , target_date)\n",
    "    result = feature.merge(target, on=['geo_cluster_id', 'sku_id', 'date'])\n",
    "    result = result.merge(price_change, on=['geo_cluster_id', 'sku_id', 'date'])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leak_cols = [f'price_change_{i}' for i in range(1,15)]\n",
    "target_cols = [f'sales_{i}' for i in range(1,15)]\n",
    "train_cols = ['geo_cluster_id', 'sku_id', 'sales', 'price_change', 'median_sales', \n",
    "              'median_non_zero_sales', 'median_price_change_sales', 'median_price_no_change_sales']\n",
    "float_cols = ['sales', 'price_change', 'median_sales', \n",
    "              'median_non_zero_sales', 'median_price_change_sales', 'median_price_no_change_sales']\n",
    "int_cols = ['geo_cluster_id', 'sku_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_valid_price_change(df):\n",
    "    df = df.copy()\n",
    "    price_change_df = np.array(df[leak_cols])\n",
    "    target_df = np.array(df[target_cols] > 0)\n",
    "    price_change_df[np.arange(target_df.shape[0]), target_df.argmax(axis=1)] = True\n",
    "    df[leak_cols] = price_change_df\n",
    "    df['price_change_1'] = False\n",
    "    \n",
    "    df.loc[~df[target_cols].any(axis=1), leak_cols] = False\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timdelta_train_days = 30\n",
    "timdelta_valid_days = 14\n",
    "\n",
    "valid_date_list = [\n",
    "    train['date'].max() - timedelta(days=14), \n",
    "    train['date'].max() - timedelta(days=21), \n",
    "    train['date'].max() - timedelta(days=28),\n",
    "    train['date'].max() - timedelta(days=35),\n",
    "    train['date'].max() - timedelta(days=42)\n",
    "]\n",
    "\n",
    "train_list = []\n",
    "valid_list = []\n",
    "for fold_valid_date in tqdm(valid_date_list):\n",
    "    fold_train = get_df(train, fold_valid_date - timedelta(days=timdelta_valid_days))\n",
    "    fold_val = get_df(train, fold_valid_date)\n",
    "    fold_val = edit_valid_price_change(fold_val)\n",
    "    train_list.append(fold_train)\n",
    "    valid_list.append(fold_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Model and Catalys stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import catalyst\n",
    "\n",
    "from catalyst import dl, metrics\n",
    "from os.path import join as pjoin\n",
    "from catalyst import callbacks\n",
    "\n",
    "from nn_utils import MetricWrapper, TableDataset, SAE, CustomRunner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForecastingFeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.sku_id_emb = nn.Embedding(len(sku_id_encoder.classes_), 128)\n",
    "        self.geo_cluster_id_emb = nn.Embedding(len(geo_cluster_id_encoder.classes_), 64)\n",
    "        self.cont_features_bn = nn.BatchNorm1d(len(float_cols) + len(leak_cols))\n",
    "        \n",
    "        n_input_features = 128 + 64 + len(float_cols) + len(leak_cols)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(n_input_features, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.PReLU(256),\n",
    "            \n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.PReLU(512),\n",
    "            \n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.PReLU(128),\n",
    "            \n",
    "            nn.Linear(128, len(target_cols)),\n",
    "        )\n",
    "        \n",
    "    def forward(self, int_f, float_f):\n",
    "        geo_cluster_id_emb = self.geo_cluster_id_emb(int_f[:,0])\n",
    "        sku_id_emb = self.sku_id_emb(int_f[:,1])\n",
    "        float_emb = self.cont_features_bn(float_f)\n",
    "        all_features = torch.cat((geo_cluster_id_emb, sku_id_emb, float_emb), dim=-1)\n",
    "        out = self.feed_forward(all_features)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 256\n",
    "EXP_NAME = \"baseline_tmax50_milr1e6_emb10xLR_lr3e4_bs256_nodrop_ver2\"\n",
    "LR = 0.0003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_metric(y_true, y_pred):\n",
    "    # 0 axis - items\n",
    "    # 1 axis - days\n",
    "    return np.mean(np.abs(y_true - y_pred).sum(0) / y_true.sum(0))\n",
    "\n",
    "def public_metric(y_true, y_pred):\n",
    "    return all_metric(y_true[:,:7], y_pred[:,:7])\n",
    "\n",
    "def private_metric(y_true, y_pred):\n",
    "    return all_metric(y_true[:,7:], y_pred[:,7:])\n",
    "\n",
    "def metric(target, predict):\n",
    "    return (target - predict).abs().sum()/target.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_fold(input_train_fold, input_val_fold, fold_id, is_test=False, epoch=None, seed=42):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    if is_test:\n",
    "        loaders = {\n",
    "            \"train\": torch.utils.data.DataLoader(\n",
    "                TableDataset(input_train_fold, float_cols=float_cols + leak_cols, int_cols=int_cols, target_cols=target_cols),\n",
    "                batch_size=BS,\n",
    "                shuffle=True,\n",
    "                drop_last=True\n",
    "            )\n",
    "        }\n",
    "    else:\n",
    "        loaders = {\n",
    "            \"train\": torch.utils.data.DataLoader(\n",
    "                TableDataset(input_train_fold, float_cols=float_cols + leak_cols, int_cols=int_cols, target_cols=target_cols),\n",
    "                batch_size=BS,\n",
    "                shuffle=True,\n",
    "                drop_last=True\n",
    "            ),\n",
    "            \"valid\": torch.utils.data.DataLoader(\n",
    "                TableDataset(input_val_fold, float_cols=float_cols + leak_cols, int_cols=int_cols, target_cols=target_cols),\n",
    "                batch_size=BS,\n",
    "                shuffle=False,\n",
    "                drop_last=False\n",
    "            )\n",
    "        }\n",
    "    model = ForecastingFeedForward()\n",
    "    criterion = SAE()\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {\"params\": model.sku_id_emb.parameters(), \"lr\": LR*10},\n",
    "        {\"params\": model.geo_cluster_id_emb.parameters(), \"lr\": LR*10},\n",
    "        {\"params\": model.cont_features_bn.parameters(), \"lr\": LR},\n",
    "        {\"params\": model.feed_forward.parameters(), \"lr\": LR},\n",
    "    ])\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=50, eta_min=1e-6\n",
    "    )\n",
    "    \n",
    "    runner = CustomRunner()\n",
    "    cat_callbacks = [\n",
    "            callbacks.OptimizerCallback(\n",
    "                metric_key=\"loss\", accumulation_steps=1\n",
    "            ),\n",
    "            callbacks.SchedulerCallback(\n",
    "                loader_key=\"train\" if is_test else \"valid\", metric_key=\"tgt_metric\", mode=\"epoch\"\n",
    "            ),\n",
    "            MetricWrapper(\n",
    "                metric_name=\"tgt_metric\",\n",
    "                metric_func=all_metric\n",
    "            ),\n",
    "            MetricWrapper(\n",
    "                metric_name=\"public_metric\",\n",
    "                metric_func=public_metric\n",
    "            ),\n",
    "            MetricWrapper(\n",
    "                metric_name=\"private_metric\",\n",
    "                metric_func=private_metric\n",
    "            )\n",
    "    ]\n",
    "    if not is_test:\n",
    "        cat_callbacks.append(callbacks.EarlyStoppingCallback(patience=7, loader_key=\"valid\", metric_key=\"tgt_metric\", minimize=True,))\n",
    "    runner.train(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        loaders=loaders,\n",
    "        logdir=pjoin(\"logdir\", EXP_NAME, f\"test_{seed}_{epoch}\") if is_test else pjoin(\"logdir\", EXP_NAME, f\"fold_{fold_id}\"),\n",
    "        valid_loader=None if is_test else \"valid\" ,\n",
    "        valid_metric=None if is_test else \"tgt_metric\",\n",
    "        minimize_valid_metric=True,\n",
    "        num_epochs=50 if epoch is None else epoch,\n",
    "        verbose=True,\n",
    "        load_best_on_end=True,\n",
    "        timeit=True,\n",
    "        callbacks=cat_callbacks,\n",
    "        seed=seed\n",
    "    )\n",
    "    val_preds = []\n",
    "    if is_test:\n",
    "         loaders = {\n",
    "            \"valid\": torch.utils.data.DataLoader(\n",
    "                TableDataset(input_val_fold, float_cols=float_cols + leak_cols, int_cols=int_cols, target_cols=target_cols),\n",
    "                batch_size=BS,\n",
    "                shuffle=False,\n",
    "                drop_last=False\n",
    "            )\n",
    "        }\n",
    "    for prediction in runner.predict_loader(loader=loaders[\"valid\"]):\n",
    "        val_preds.append(prediction)\n",
    "    return np.concatenate(val_preds, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(index=range(len(train_list)), columns=target_cols)\n",
    "for i, (fold_valid, fold_train) in enumerate(zip(valid_list, train_list)):\n",
    "    #fold_train = fold_train.iloc[:10_000]\n",
    "    #fold_valid = fold_valid.iloc[:10_000]\n",
    "    val_forecast = train_one_fold(fold_train, fold_valid, fold_id=i)\n",
    "    for target_col_id, target_col in enumerate(target_cols):\n",
    "        results.loc[i, target_col] = metric(fold_valid[target_col], val_forecast[:,target_col_id])\n",
    "        \n",
    "print('public:', results[['sales_1', 'sales_2', 'sales_3', 'sales_4', 'sales_5', 'sales_6', 'sales_7']].mean().mean())\n",
    "print('private:', results[['sales_8', 'sales_9', 'sales_10', 'sales_11', 'sales_12', 'sales_13', 'sales_14']].mean().mean())\n",
    "print(pd.DataFrame(results.mean(axis=1)).T, '\\n')\n",
    "print(pd.DataFrame(results.mean(axis=0)).T, '\\n')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('public:', results[['sales_1', 'sales_2', 'sales_3', 'sales_4', 'sales_5', 'sales_6', 'sales_7']].mean().mean())\n",
    "print('private:', results[['sales_8', 'sales_9', 'sales_10', 'sales_11', 'sales_12', 'sales_13', 'sales_14']].mean().mean())\n",
    "print(pd.DataFrame(results.mean(axis=1)).T, '\\n')\n",
    "print(pd.DataFrame(results.mean(axis=0)).T, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train = get_df(train, train['date'].max()-timedelta(days=14))\n",
    "\n",
    "test_test_feature = get_feature(train, train['date'].max())\n",
    "test_test_price_change = get_price_change(test.append(train).sort_values(['geo_cluster_id', 'sku_id', 'date']), train['date'].max())\n",
    "test_test = test_test_feature.merge(test_test_price_change, on=['geo_cluster_id', 'sku_id', 'date'])\n",
    "test_test[target_cols] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_forecasts = []\n",
    "for s_v in [42, 2021, 12345]:\n",
    "    for e_v in [16, 18, 19]:\n",
    "        test_forecast = train_one_fold(test_train, test_test, fold_id=0, is_test=True, epoch=e_v, seed=s_v)\n",
    "        test_forecasts.append(test_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_forecasts = np.stack(test_forecasts, axis=0).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = []\n",
    "for target_col_id, (target_col, date) in enumerate(zip(tqdm(target_cols), np.sort(test['date'].unique()))):\n",
    "    pred = test_test[['geo_cluster_id', 'sku_id']].copy()\n",
    "    pred['date'] = date\n",
    "    pred['pred'] = test_forecasts[:,target_col_id]\n",
    "    test_predictions.append(pred)\n",
    "    \n",
    "test_predictions = pd.concat(test_predictions).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../data/raw/sample_final.csv')\n",
    "test_predicted = test.reset_index().merge(test_predictions, on=['geo_cluster_id', 'sku_id', 'date'], how='left').fillna(0)\n",
    "sub['sales'] = test_predicted['pred'].tolist()\n",
    "sub.loc[sub['sales'] < 0, 'sales'] = 0\n",
    "sub['sales'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(f'../submissions/{EXP_NAME}_9models_leshapreproc.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
